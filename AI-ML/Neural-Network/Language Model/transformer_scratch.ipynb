{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa15cea5-1ba4-4b23-a450-b1805adf078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import ModuleList\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import reduce, rearrange, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d67a6baa-9ae2-4027-9476-05d2aea5ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f118247-0849-4196-b80a-7c3c72c5598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380293a3-420d-42d1-8696-e1d19d260c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/Harry_Potter.txt\", 'r')\n",
    "content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea1c569-48a6-4a99-aca0-76a3dc8e882e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442745"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d784362-2c4e-46f4-8de0-2e1220432b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41209e5d-ef56-4c69-b38a-0b25c2dd4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ff55e7f-312d-4104-8f0a-c9d71fc28f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02a5fbb3-02a7-4976-86c7-9c168ae5c0ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"After each block, the token tensor is normalized along it's dimension.\", \"The normalization itself is a learnable module.\"],\n",
    "         padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7ff3edca-e8c8-41ef-a6a2-3d47efcf9a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3260,  1123,  2512,    11,   262, 11241, 11192,   273,   318, 39279,\n",
       "          1863,   340,   338, 15793,    13],\n",
       "        [  464,  3487,  1634,  2346,   318,   257,  2193,   540,  8265,    13,\n",
       "         50257, 50257, 50257, 50257, 50257]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d3627fa-f678-4c1f-9544-e83bd27a2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    def __init__(self, data, seq_len, tokenizer):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.num_seq = len(self.data)//self.seq_len\n",
    "        self.data = [self.data[i:i+self.seq_len] for i in range(0, self.num_seq, self.seq_len)]\n",
    "        self.data = tokenizer(self.data, padding=True, truncation=True, return_tensors='pt')\n",
    "        self.sample_size = len(self.data)\n",
    "    def __len__(self):\n",
    "        return self.sample_size\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[\"input_ids\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a40cd92c-d264-43fd-b57a-a694f0a61999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After each block, the token tensor is normalized along it's dimension.\n",
    "The normalization itself is a learnable module.\n",
    "\"\"\"\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1.e-5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.w = nn.Parameter(th.ones(self.dim))\n",
    "        self.b = nn.Parameter(th.zeros(self.dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        #mean and standard deviation along the dimension of token vector\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = (x.var(dim=-1, keepdim=True, unbiased=False) + self.eps).sqrt()\n",
    "        return self.w*(x - mean)/std + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08b5dae3-8c5f-4cb2-a9b5-59b6595f8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding block consists of a lookup matrix.\n",
    "The matrix consists of embedding vector for each token in the vocabulary\n",
    "\"\"\"\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Parameter(th.empty(self.vocab_dim, self.embed_dim))\n",
    "        nn.init.normal_(self.embedding, std=1.)\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding[tokens-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52dd2403-472b-432f-af25-978c5025abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The positional embedding is calculated for once for the maximum context size and stored as a buffer.\n",
    "Each token in the sequence is assigned an array of sequence of values of sine and cosine. The positional embedding adds to the input tensor.\n",
    "\"\"\"\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        #timestamp for each token\n",
    "        t = th.arange(max_seq_len).reshape(-1,1)\n",
    "        #each timestamp has a dimension which contains sequence of sine and cosine values\n",
    "        se = th.arange(0, dim, 2).reshape(1,-1)\n",
    "        ce = th.arange(0, dim if dim%2==0 else dim-1, 2).reshape(1,-1)\n",
    "        se = th.exp(-8*se*math.log(10)/dim)\n",
    "        ce = th.exp(-8*ce*math.log(10)/dim)\n",
    "        se = (t*se).sin()\n",
    "        ce = (t*ce).cos()\n",
    "        encoding = th.zeros(max_seq_len, dim)\n",
    "        #the sine and cosine sequence of values are interleaved together\n",
    "        encoding[:,0::2] = se\n",
    "        encoding[:,1::2] = ce\n",
    "        self.register_buffer(\"encoding\", encoding)\n",
    "    def forward(self, x):\n",
    "        #x.shape = (batch_size, seq_len, dim)\n",
    "        seq_len = x.shape[1]\n",
    "        return x + self.encoding[:seq_len,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b7605f-63db-45c2-bffe-1cdc4b7c90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes the batched input of sequence of vectors of shape (batch_size, seq_len, embed_dim).\n",
    "Each head independently calculates the masked attention pattern for the batch of sequence.\n",
    "The output vector is weighted according to the attention patterns.\n",
    "The output sequence of vectors are concatenated along the vector dimension.\n",
    "The concatenated sequence of vectors are linear transformed to produce the output.\n",
    "'''\n",
    "class Attention(th.nn.Module):\n",
    "    def __init__(self, num_heads, res_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.res_dim = res_dim\n",
    "        assert self.res_dim%self.num_heads == 0\n",
    "        self.head_dim = self.res_dim//self.num_heads\n",
    "        self.scale = 1/math.sqrt(self.head_dim)\n",
    "        self.layer_norm = LayerNorm(self.res_dim)\n",
    "        #define a linear layer that map to three times the residual dimension\n",
    "        #the resulting vector produced will be the concatenation of query, key and value vectors\n",
    "        self.QKV = th.nn.Linear(self.res_dim, 3*self.res_dim)\n",
    "        #the concatenated output from heads will be transformed using this\n",
    "        self.O = th.nn.Linear(self.res_dim, self.res_dim)\n",
    "    def forward(self, x):\n",
    "        #x.shape = (batch_size, seq_len, res_dim)\n",
    "        #q,k,v.shape = (batch_size, num_heads, seq_len, head_dim)\n",
    "        x = self.layer_norm(x)\n",
    "        q, k, v = rearrange(self.QKV(x), 'b s (qkv h d) -> qkv b h s d', qkv=3, h=self.num_heads, d=self.head_dim)\n",
    "        attn_patterns = self.attnPatterns(q, k)\n",
    "        weighted_v = self.weightedValues(attn_patterns, v)\n",
    "        output_v = rearrange(weighted_v, 'b h s d -> b s (h d)')\n",
    "        return self.O(output_v)\n",
    "    #q, k - (batch_size, num_heads, seq_len, head_dim)\n",
    "    #attention patterns are stored row-wise for each sequence element\n",
    "    def attnScores(self, q, k):\n",
    "        return einsum(q, k, '... s1 d1, ... s2 d1 -> ... s1 s2')*self.scale\n",
    "    def attnPatterns(self, q, k):\n",
    "        attn_scores = self.attnScores(q, k)\n",
    "        dim = attn_scores.shape[-1]\n",
    "        IGNORE = -1.e6\n",
    "        a = th.arange(dim).reshape(-1,1)\n",
    "        b = th.arange(dim).reshape(1,-1)\n",
    "        mask = a<b\n",
    "        attn_scores[..., mask] = IGNORE\n",
    "        return th.softmax(attn_scores, dim=-1)\n",
    "    def weightedValues(self, attn_patterns, values):\n",
    "        return einsum(attn_patterns, values, '... qpos seq, ... seq dim -> ... qpos dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a353dbbd-f866-461e-8452-93771b1b08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MLP block consists of two linear layers, an activation, and a layer norm in the final layer.\n",
    "\"\"\"\n",
    "\n",
    "class MLPBlock(th.nn.Module):\n",
    "    def __init__(self, res_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.res_dim = res_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.l1 = th.nn.Linear(self.res_dim, self.hidden_dim)\n",
    "        self.l2 = th.nn.Linear(self.hidden_dim, self.res_dim)\n",
    "        self.act = th.nn.GELU()\n",
    "        self.layer_norm = LayerNorm(self.res_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4af580-8fe9-4e2d-9b6a-108052b2539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The transformer block consists of an attention and an MLP blocks.\n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(th.nn.Module):\n",
    "    def __init__(self, num_heads, res_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.res_dim = res_dim\n",
    "        self.hidden_dim = 4*self.res_dim\n",
    "        self.attention = Attention(self.num_heads, self.res_dim)\n",
    "        self.mlp = MLPBlock(self.res_dim, self.hidden_dim)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc33ea7d-8e4e-4a84-b138-24df331412ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The training function takes the text file.\n",
    "The forward method takes a batch of tokenized sequence.\n",
    "Converts the text to a batch of sequences.\n",
    "Tokenize the batch, embed it, and add positional embedding.\n",
    "The resulting tensor is passed to transformer blocks successively.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Transformer(th.nn.Module):\n",
    "    def __init__(self, num_layers=2, num_heads=12, res_dim=784, vocab_dim):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.res_dim = res_dim\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.max_seq_len = 1024\n",
    "        assert self.res_dim%self.num_heads == 0\n",
    "        \n",
    "        self.embedding = Embedding(self.vocab_dim, self.res_dim)\n",
    "        self.pos_encoding = PosEncoding(self.res_dim, self.max_seq_len)\n",
    "        self.layers = ModuleList(\n",
    "            [TransformerBlock(self.num_heads, self.res_dim)\n",
    "            for i in range(self.num_layers)])\n",
    "        self.out_layer = th.nn.Linear(self.res_dim, self.vocab_dim)\n",
    "    def setup(self, batch_size, epochs, lr=1.e-3):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.out_layer(x)\n",
    "    def trainStep(self, dataset):\n",
    "        data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "        for batch in data_loader:\n",
    "            pred_logits = self(batch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b41343-7129-4630-b76e-bea8b5949cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
