{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc0c75f-4cb6-474f-98b1-0ac82b2f9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43b2811-f48d-4aa2-9767-e68253f5de71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN(th.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_embeddings=None, embedding_dim=None):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.in_embedding = th.nn.Embedding(self.num_embeddings, self.embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.in_dim = self.embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.i2h = th.nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        self.h2h = th.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.out_dim = self.num_embeddings\n",
    "        self.h2o = th.nn.Linear(self.hidden_dim, self.out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, seq_len, in_dim)\n",
    "        in_seq = self.in_embedding(x)\n",
    "        \n",
    "        seq_len = in_seq.shape[1]\n",
    "        batch_size = in_seq.shape[0]\n",
    "        hidden = th.zeros(batch_size, self.hidden_dim)\n",
    "        hidden_seq = th.zeros(batch_size, seq_len, self.hidden_dim)\n",
    "        out_seq = th.zeros(batch_size, seq_len, self.out_dim)\n",
    "        \n",
    "        for seq_step in range(seq_len):\n",
    "            hidden = th.tanh(self.i2h(in_seq[:, seq_step, :]) + self.h2h(hidden))\n",
    "            hidden_seq[:, seq_step, :] = hidden\n",
    "            out_seq[:, seq_step, :] = self.h2o(hidden)\n",
    "\n",
    "        return hidden_seq, out_seq\n",
    "\n",
    "    @th.no_grad()\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        hidden_seq, out_seq = self(x)\n",
    "        return hidden_seq, F.softmax(out_seq, dim=-1)\n",
    "        \n",
    "    def setup(self, learning_rate=1.e-3, batch_size=1, epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.criterion = th.nn.CrossEntropyLoss()\n",
    "        self.optimizer = th.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def train_step(self, dataset):\n",
    "        history = th.zeros(epochs)\n",
    "        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.train()\n",
    "            for in_train, out_train in data_loader:\n",
    "                _, pred_seq = self(in_train)\n",
    "                pred_seq = pred_seq.view(-1, self.num_embeddings)\n",
    "                out_train = out_train.view(-1)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(pred_seq, out_train)\n",
    "                loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                history[epoch] = loss\n",
    "            \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0fdfa9-f135-4e2a-a8e7-9442edef6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(th.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_dim = hidden_dim\n",
    "        \n",
    "        self.forget_gate_i2c = th.nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
    "        self.forget_gate_h2c = th.nn.Linear(self.hidden_dim, self.cell_dim)\n",
    "        \n",
    "        self.input_gate_i2c = th.nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
    "        self.input_gate_h2c = th.nn.Linear(self.hidden_dim, self.cell_dim)\n",
    "        \n",
    "        self.cell_state_i2c = th.nn.Linear(self.in_dim, self.cell_dim, bias=False)\n",
    "        self.cell_state_h2c = th.nn.Linear(self.hidden_dim, self.cell_dim)\n",
    "        \n",
    "        self.out_gate_i2h = th.nn.Linear(self.in_dim, self.hidden_dim, bias=False)\n",
    "        self.out_gate_h2h = th.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "    def forget_gate(self, x, hidden):\n",
    "        return th.sigmoid(self.forget_gate_i2c(x) + self.forget_gate_h2c(hidden))\n",
    "    def input_gate(self, x, hidden):\n",
    "        return th.sigmoid(self.input_gate_i2c(x) + self.input_gate_h2c(hidden))\n",
    "    def output_gate(self, x, hidden):\n",
    "        return th.sigmoid(self.out_gate_i2h(x) + self.out_gate_h2h(hidden))\n",
    "        \n",
    "    def lstm_step(self, x, hidden, cell_state):\n",
    "        candidate_cell_state = th.tanh(self.cell_state_i2c(x) + self.cell_state_h2c(hidden))\n",
    "        updated_cell_state = cell_state*self.forget_gate(x, hidden) + candidate_cell_state*self.input_gate(x, hidden)\n",
    "        updated_hidden = th.tanh(updated_cell_state)*self.output_gate(x, hidden)\n",
    "        \n",
    "        return updated_hidden, updated_cell_state\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        cell_state = th.zeros(batch_size, self.cell_dim)\n",
    "        hidden = th.zeros(batch_size, self.hidden_dim)\n",
    "        hidden_seq = th.zeros(batch_size, seq_len, self.hidden_dim)\n",
    "        \n",
    "        for seq_step in range(seq_len):\n",
    "            cell_state, hidden = self.lstm_step(x[:, seq_step,:], hidden, cell_state)\n",
    "            hidden_seq[:, seq_step,:] = hidden\n",
    "            \n",
    "        return hidden_seq, hidden_seq[:, -1,:]\n",
    "        \n",
    "    @th.no_grad()\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        return self(x)\n",
    "        \n",
    "    def setup(self, lr=1.e-3, momentum=0.9, epochs=10):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.criterion = th.nn.MSELoss(reduction=\"mean\")\n",
    "        self.optimizer = th.optim.SGD(self.parameters(), lr=lr, momentum=momentum)\n",
    "        \n",
    "    def train_step(self, x_train, target, hidden=None):\n",
    "        history = th.zeros(self.epochs)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.train()\n",
    "            pred_seq, _ = self(x_train)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(pred_seq, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            history[epoch] = loss\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08533e-a909-4de6-8d1c-878f04ce1432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c411413-ecc4-47ba-956a-7df46d74a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class copyingTaskDataset(Dataset):\n",
    "    def __init__(self, seq_len, num_fields2rem, num_samples, num_letters=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_letters = num_letters\n",
    "        self.num_fields2rem = num_fields2rem\n",
    "        self.num_samples = num_samples\n",
    "        self.fields2rem = th.zeros(self.num_fields2rem)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        in_seq = th.zeros(self.seq_len, dtype=int)\n",
    "        out_seq = th.zeros(self.seq_len, dtype=int)\n",
    "        fields2rem = th.randint(2, num_letters, size=(self.num_fields2rem,))\n",
    "        \n",
    "        in_pos_fields = th.randint(0, self.seq_len-2*self.num_fields2rem+1, size=())\n",
    "        in_seq[in_pos_fields:in_pos_fields+self.num_fields2rem] = fields2rem\n",
    "        \n",
    "        out_pos_fields = th.randint(in_pos_fields+self.num_fields2rem, self.seq_len-self.num_fields2rem+1, size=())\n",
    "        out_seq[out_pos_fields: out_pos_fields+self.num_fields2rem] = fields2rem\n",
    "        in_seq[out_pos_fields] = 1\n",
    "        \n",
    "        return in_seq, out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44836fcc-5667-4c62-9fdb-3e1d3d62447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 15\n",
    "num_fields2rem = 1\n",
    "num_samples = 5000\n",
    "batch_size = 400\n",
    "epochs = 50\n",
    "num_letters = 10\n",
    "\n",
    "dataset = copyingTaskDataset(seq_len, num_fields2rem, num_samples, num_letters=num_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e35f1e-35f0-4476-8e3f-6bfe7e97a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "hidden_dim = 50\n",
    "\n",
    "rnn = RNN(hidden_dim, num_embeddings=num_letters+1, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c4157b6-c88a-4700-b62b-92e715e6891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.e-3\n",
    "\n",
    "rnn.setup(learning_rate=learning_rate, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "dc7a7cf2-3735-4bd4-b130-845399dbd2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6576, 0.7244, 0.5019, 0.3995, 0.3329, 0.2958, 0.2704, 0.2518, 0.2364,\n",
       "        0.2264, 0.2170, 0.2118, 0.2026, 0.1963, 0.1916, 0.1870, 0.1821, 0.1788,\n",
       "        0.1747, 0.1727, 0.1689, 0.1656, 0.1627, 0.1607, 0.1588, 0.1571, 0.1556,\n",
       "        0.1537, 0.1531, 0.1512, 0.1497, 0.1491, 0.1480, 0.1466, 0.1454, 0.1442,\n",
       "        0.1427, 0.1424, 0.1414, 0.1400, 0.1397, 0.1367, 0.1372, 0.1359, 0.1345,\n",
       "        0.1315, 0.1280, 0.1203, 0.1123, 0.1076], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.train_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8a5684af-102a-4902-990c-4f22121b0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 10\n",
    "test_dataset = copyingTaskDataset(seq_len, num_fields2rem, num_test_samples, num_letters=num_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "daf73698-d8ac-4043-92bd-a02ab156e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "526c4e2c-642e-45ff-a3a4-f1e4f615c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_in\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 0]])\n",
      "test_out\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0]])\n",
      "test_pred\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "test_in, test_out = next(iter(test_data_loader))\n",
    "\n",
    "test_pred = rnn.predict(test_in)[1]\n",
    "\n",
    "print(\"test_in\")\n",
    "print(test_in)\n",
    "print(\"test_out\")\n",
    "print(test_out)\n",
    "print(\"test_pred\")\n",
    "#print(test_pred)\n",
    "print(th.argmax(test_pred, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40559c09-3e68-4055-aa24-be41dfc89e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3ab86-25dc-4533-bb49-4785104e9d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055824ca-9c3c-4ab7-8017-0c4ef3581611",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 5\n",
    "hidden_dim = 10\n",
    "\n",
    "lstm = LSTM(in_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32b22f0-6057-4a8a-8e81-a6f2e5d4c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.e-3\n",
    "epochs = 100\n",
    "\n",
    "lstm.setup(lr=learning_rate, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136cac9-703a-4598-a45f-4ffd0dd34663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
